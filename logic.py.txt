import random
from transformers import pipeline
from utils import split_text

from langchain_community.vectorstores import FAISS
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain_community.embeddings import HuggingFaceEmbeddings
from langchain_community.llms import HuggingFacePipeline


def create_vector_db(document_text):
    chunks = split_text(document_text)
    embeddings = HuggingFaceEmbeddings(model_name="sentence-transformers/all-MiniLM-L6-v2")
    vectordb = FAISS.from_texts(chunks, embedding=embeddings)
    return vectordb, chunks


# âš¡ Use a local model via pipeline
text2text_generator = pipeline(
    "text2text-generation",
    model="google/flan-t5-large",
    tokenizer="google/flan-t5-large"
)

llm = HuggingFacePipeline(pipeline=text2text_generator)


def generate_challenge_question(chunks, level="easy"):
    chunk = random.choice(chunks)

    question_prompt = {
        "easy": "What does this paragraph imply?",
        "medium": "Summarize this concept and apply it to a real-world dataset.",
        "hard": "Critically evaluate the assumptions or methods described here."
    }[level]

    prompt = f"{question_prompt}\n\n{chunk}\n\nAnswer:"
    answer = llm(prompt)

    return f"Question: {question_prompt}\n\nContext:\n\"{chunk}\"\n\nAnswer: {answer}"
